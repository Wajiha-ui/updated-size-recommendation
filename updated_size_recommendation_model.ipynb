{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt2WqwEBjdY1l86v2PTZiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wajiha-ui/updated-size-recommendation/blob/main/updated_size_recommendation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-IKmnTgaKsh",
        "outputId": "69b6dd9e-c373-4cc3-cab4-c81dab6a99eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š RandomForest Accuracy: 60.14%\n",
            "ðŸ“Š XGBoost Accuracy: 56.27%\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003237 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6512, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607444\n",
            "[LightGBM] [Info] Start training from score -1.613593\n",
            "[LightGBM] [Info] Start training from score -1.612052\n",
            "[LightGBM] [Info] Start training from score -1.619780\n",
            "[LightGBM] [Info] Start training from score -1.594501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š LightGBM Accuracy: 51.54%\n",
            "ðŸ“Š NeuralNetwork Accuracy: 37.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 6512, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607444\n",
            "[LightGBM] [Info] Start training from score -1.613593\n",
            "[LightGBM] [Info] Start training from score -1.612052\n",
            "[LightGBM] [Info] Start training from score -1.619780\n",
            "[LightGBM] [Info] Start training from score -1.594501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607328\n",
            "[LightGBM] [Info] Start training from score -1.614056\n",
            "[LightGBM] [Info] Start training from score -1.611167\n",
            "[LightGBM] [Info] Start training from score -1.619859\n",
            "[LightGBM] [Info] Start training from score -1.594953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607328\n",
            "[LightGBM] [Info] Start training from score -1.614056\n",
            "[LightGBM] [Info] Start training from score -1.612129\n",
            "[LightGBM] [Info] Start training from score -1.618889\n",
            "[LightGBM] [Info] Start training from score -1.594953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001976 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5210, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607520\n",
            "[LightGBM] [Info] Start training from score -1.613284\n",
            "[LightGBM] [Info] Start training from score -1.612321\n",
            "[LightGBM] [Info] Start training from score -1.620051\n",
            "[LightGBM] [Info] Start training from score -1.594200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5210, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607520\n",
            "[LightGBM] [Info] Start training from score -1.613284\n",
            "[LightGBM] [Info] Start training from score -1.612321\n",
            "[LightGBM] [Info] Start training from score -1.620051\n",
            "[LightGBM] [Info] Start training from score -1.594200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001568 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1115\n",
            "[LightGBM] [Info] Number of data points in the train set: 5210, number of used features: 28\n",
            "[LightGBM] [Info] Start training from score -1.607520\n",
            "[LightGBM] [Info] Start training from score -1.613284\n",
            "[LightGBM] [Info] Start training from score -1.612321\n",
            "[LightGBM] [Info] Start training from score -1.620051\n",
            "[LightGBM] [Info] Start training from score -1.594200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Stacking Ensemble Accuracy: 61.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Best model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Generate synthetic dataset\n",
        "def generate_synthetic_data(num_entries=5000):\n",
        "    data = []\n",
        "    genders = [\"Male\", \"Female\"]\n",
        "    age_groups = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55+\"]\n",
        "    body_shapes = [\"Slim\", \"Athletic\", \"Regular\", \"Plus-size\"]\n",
        "    fit_preferences = [\"Slim\", \"Regular\", \"Loose\"]\n",
        "    clothing_types = [\"T-Shirt\", \"Hoodie\", \"Dress\", \"Jacket\", \"Pants\"]\n",
        "    materials = [\"Cotton\", \"Wool\", \"Silk\", \"Polyester\", \"Linen\"]\n",
        "    sizes = [\"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\"]\n",
        "    regions = [\"Europe\", \"USA\", \"Asia\"]\n",
        "\n",
        "    for _ in range(num_entries):\n",
        "        data.append({\n",
        "            \"Gender\": random.choice(genders),\n",
        "            \"Age Group\": random.choice(age_groups),\n",
        "            \"Height (cm)\": random.randint(150, 200),\n",
        "            \"Weight (kg)\": random.randint(45, 120),\n",
        "            \"Chest (cm)\": random.randint(75, 130),\n",
        "            \"Waist (cm)\": random.randint(60, 110),\n",
        "            \"Hips (cm)\": random.randint(80, 130),\n",
        "            \"Body Shape\": random.choice(body_shapes),\n",
        "            \"Preferred Fit\": random.choice(fit_preferences),\n",
        "            \"Clothing Type\": random.choice(clothing_types),\n",
        "            \"Brand Size\": random.choice(sizes),\n",
        "            \"Material Preference\": random.choice(materials),\n",
        "            \"Country/Region\": random.choice(regions)\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate and preprocess dataset\n",
        "df = generate_synthetic_data(5000)\n",
        "df[\"BMI\"] = df[\"Weight (kg)\"] / ((df[\"Height (cm)\"] / 100) ** 2)\n",
        "df[\"Chest_Waist_Ratio\"] = df[\"Chest (cm)\"] / df[\"Waist (cm)\"]\n",
        "df[\"Waist_Hips_Ratio\"] = df[\"Waist (cm)\"] / df[\"Hips (cm)\"]\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Identify the one-hot encoded 'Brand Size' columns\n",
        "brand_size_cols = [col for col in df.columns if col.startswith(\"Brand Size_\")]\n",
        "X = df.drop(columns=brand_size_cols)\n",
        "y = df[brand_size_cols].idxmax(axis=1).str.replace(\"Brand Size_\", \"\")\n",
        "\n",
        "# Encode target labels numerically\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Balance the dataset with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "drf_model = RandomForestClassifier(n_estimators=500, max_depth=30, random_state=42)\n",
        "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), learning_rate=0.05, max_depth=10, n_estimators=300, random_state=42)\n",
        "lgb_model = lgb.LGBMClassifier(num_leaves=31, learning_rate=0.05, n_estimators=300, random_state=42)\n",
        "nn_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "\n",
        "# Train models\n",
        "models = {\"RandomForest\": drf_model, \"XGBoost\": xgb_model, \"LightGBM\": lgb_model, \"NeuralNetwork\": nn_model}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ðŸ“Š {name} Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Stacking Model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[(\"RandomForest\", drf_model), (\"XGBoost\", xgb_model), (\"LightGBM\", lgb_model)],\n",
        "    final_estimator=nn_model\n",
        ")\n",
        "\n",
        "# Train and evaluate stacking model\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"ðŸ”— Stacking Ensemble Accuracy: {stacking_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save best model\n",
        "best_model = max(models.items(), key=lambda x: accuracy_score(y_test, x[1].predict(X_test)))[1]\n",
        "joblib.dump(best_model, \"best_size_model.pkl\")\n",
        "print(\"âœ… Best model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_model, \"best_size_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXuQuEhpb32P",
        "outputId": "3fdd94e3-465b-4eb9-f3d8-d38dbab4d1ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_size_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"best_size_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZBtfIMOYcKMk",
        "outputId": "01f74d92-e08c-4c0b-e376-a8dfb284dc4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_668f8756-8542-43c8-9773-3414aaa9c435\", \"best_size_model.pkl\", 240100705)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}